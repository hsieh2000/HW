{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC/yxQJYUVWBVbqJ6sqwHS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsieh2000/HW/blob/main/foreign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#匯入套件\n",
        "import time \n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import json \n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from fake_useragent import UserAgent\n",
        "import random\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "#讀取config.json\n",
        "config_path = \"../config.json\" #預設路徑\n",
        "with open(config_path) as f:   #開啟config.json\n",
        "    config = json.load(f)      #以json形式讀取config.json\n",
        "    root = config[\"root\"]      #取得檔案輸出根目錄\n",
        "\n",
        "\n",
        "# 建立資料夾\n",
        "newest_path = f'{root}最新檔/'                  #設定最新檔匯出路徑\n",
        "log_path = f'{root}境外基金基本資訊記錄檔/'     #設定紀錄檔匯出路徑\n",
        "dir = [\"csv/\", \"txt/\", \"err/\"]                #設定檔案存放資料夾清單\n",
        "\n",
        "for i in dir:                                 #以for迴圈方式建立資料夾\n",
        "  if not os.path.isdir(log_path+i):            #if判斷資料夾是否已經存在\n",
        "    os.makedirs(log_path+i)                   #如果不存在就新增\n",
        "\n",
        "if not os.path.isdir(newest_path):            #if判斷資料夾是否已經存在\n",
        "    os.mkdir(newest_path)                     #如果不存在就新增\n",
        "\n",
        "# 路徑設定\n",
        "filename = \"境外基金基本資訊\"                   #設定輸出檔名                 \n",
        "bug_url = \"境外基金錯誤連結\"\t\t            #設定錯誤連結檔名\n",
        "csv_path = log_path+dir[0]                     #設定csv檔輸出路徑 \n",
        "txt_path = log_path+dir[1]                    #設定txt檔輸出路徑 \n",
        "err_path = log_path+dir[2]        \t            #設定錯誤連結輸出路徑 \n",
        "\n",
        "# 參數設定\n",
        "delay = random.uniform(1, 5)                  #設定隨機數1至5\n",
        "warnings.filterwarnings(\"ignore\")             #忽略警告訊息\n",
        "user_agent = UserAgent()                      #調用 UserAgent() 使用者代理\n",
        "\n",
        "header = {\"user-agent\" : user_agent.random,\n",
        "      \"Referer\" : \"https://announce.fundclear.com.tw/MOPSFundWeb/E03_01.jsp\"}  #設定爬蟲header\n",
        "\n",
        "link_header = 'https://announce.fundclear.com.tw/MOPSFundWeb/'                 #設定連結表頭(可刪\n",
        "\n",
        "# 資料月份判斷\n",
        "if int(datetime.datetime.now().strftime(\"%d\"))>10:                             #if判斷本月日期是否大於10號\n",
        "    month = datetime.datetime.now() - relativedelta(months=1)                  #為真則將當前月份-1存入month變數\n",
        "    month = month.strftime(\"%Y%m\")                                             #將month變數轉換為\"西元年月\"格式的字串\n",
        "else:                                                                          #else\n",
        "    month = datetime.datetime.now() - relativedelta(months=2)                  #為假則將當前月份-2存入month變數\n",
        "    month = month.strftime(\"%Y%m\")                                             #將month變數轉換為\"西元年月\"格式的字串\n",
        "\n",
        "\n",
        "\n",
        "class newForeignFund(object):                                                     #定義名為newForeignFund的類別\n",
        "\n",
        "    def __init__(self):                                                           #宣告 def __init__()\n",
        "        with open(config_path) as f:                                              #開啟config.json檔\n",
        "            config = json.load(f)                                                 #以json形式讀取config.json\n",
        "            target_agent_list = config['agent']                                   #讀取config.json中的agent中的目標機構代碼                                         \n",
        "            print(target_agent_list)                                              #印出目標投信機構\n",
        "            with open(f'{txt_path}{filename}{month}.txt', mode='w') as T:         #在txt的資料夾下新增一個檔名為\"境外基金基本資訊+西元年月\"的txt檔，模式為寫入\n",
        "                T.write('{\\'funds\\':[')                                           #寫入'{\\'funds\\':['以符合json格式\n",
        "\n",
        "        a = self.general_agent()                                                  #執行general_agent() 並將結果存入變數a\n",
        "        a = self.compare(a,target_agent_list)                                     #執行compare() 並將結果存入變數a\n",
        "        a = self.tab_link_generaltor(a)                                           #執行tab_link_generaltor() 並將結果存入變數a\n",
        "        a = self.meta_link_generator(a)                                           #執行meta_link_generator() 並將結果存入變數a\n",
        "        a = self.meta_crawler(a)                                                  #執行meta_crawler() 並將結果存入變數a\n",
        "        a = self.toDataframe(a)                                                   #執行toDataframe() 並將結果存入變數a\n",
        "        a = self.store(a)                                                         #執行store() 並將結果存入變數a\n",
        "\n",
        "\n",
        "    def general_agent(self):                                                                                                #宣告 general_agent()\n",
        "        url = 'https://announce.fundclear.com.tw/MOPSFundWeb/O01.jsp?organizType=ALL&organizCom=ALL&fundCom=ALL'            #將基金觀測站的機構列表頁網址存入變數url\n",
        "        header = {\"user-agent\" : user_agent.random}                                                                         ####可刪\n",
        "        res=requests.get(url,headers = header, timeout=30)                                                                  #取得網頁原始內容，並設定請求時間上限為30秒\n",
        "        soup = BeautifulSoup(res.text, 'html.parser')                                                                       #透過BeautifulSoup解析原始內容，解析模式為\"html.parser\"\n",
        "        webText = str(soup.select(\"select[name='organizCom'] option\"))                                                      #選取name='organizCom'的所有option，並將結果轉換為字串存入webText變數\n",
        "        regex = re.compile('\\w\\d{4}\">\\w+')                                                                                  #定義正規表達式'\\w\\d{4}\">\\w+'並存入regex變數，目的為過濾出所有機構名稱與代碼\n",
        "        result = re.findall(regex, webText)                                                                                 #透過re.findall找出符合regex規則的所有結果並存入result變數\n",
        "        result = result[2:]                                                                                                 #由於result的前兩項固定為不相干內容，因此去除想兩項結果\n",
        "        agent_name = list(map(lambda x : x.split('\">')[1],result))                                                          #對result內元素以\">\"為中心進行拆分並選擇後半段作為機構名稱，將結果存回agent_name\n",
        "        agent_ID = list(map(lambda x : x.split('\">')[0],result))                                                            #對result內元素以\">\"為中心進行拆分並選擇前半段作為機構名稱，將結果存回agent_ID\n",
        "        print(agent_name)                                                                                                   #印出全數機構名稱\n",
        "        return agent_ID                                                                                                     #返回agent_ID\n",
        "\n",
        "    def compare(self, agent_ID, target_list):                                                                               #宣告 compare()，此function需要輸入兩個參數，分別為全部機構代碼列表與目標機構代碼列表\n",
        "        # target_list = ['A0038','A0031','A0018','B0029','A0036','A0011','A0032']\n",
        "        target_Id = list(set(target_list).intersection(set(agent_ID)))                                                      #取得全部機構代碼列表與目標機構代碼列表的交集，並將結果存入target_Id變數\n",
        "        print(target_Id)                                                                                                    #印出有效的目標機構代碼列表\n",
        "        return target_Id                                                                                                    #返回有效的目標機構代碼列表\n",
        "\n",
        "    def tab_link_generaltor(self, target_Id):                                                                               #宣告 tab_link_generaltor()，此function需要輸入一個變數，為有效目標機構代碼列表\n",
        "        regex2 = re.compile(\"A01_01.jsp?.+establishYear=ALL\")                                                               #定義正規表達式'A01_01.jsp?.+establishYear=ALL'並存入regex2變數，目的為生成各機構的銷售基金總覽網址\n",
        "        link_header = 'https://announce.fundclear.com.tw/MOPSFundWeb/'                                                      #設定連結表頭\n",
        "        target_link_list=[]                                                                                                 #宣告一個名為target_link_list的空list\n",
        "\n",
        "        for ID in target_Id:                                                                                                                                #透過for迴圈將機構ID帶入網址模板，取得各機構的基金總覽網址       \n",
        "            agent_url = f'https://announce.fundclear.com.tw/MOPSFundWeb/O01.jsp?organizType=ALL&organizCom={ID}&fundCom=ALL'\n",
        "            res=requests.get(agent_url,headers = header, timeout=10)\n",
        "            soup = BeautifulSoup(res.text, 'html.parser')\n",
        "            AgentWebText = soup.select('td[style=\"cursor: hand; color: blue\"]')\n",
        "            link = [ regex2.search(str(s)).group(0) for s in AgentWebText if regex2.search(str(s)) ]\n",
        "            link = list(map(lambda x : link_header+x.replace(';','&').replace('®','&reg').replace('establishYear=ALL','establishYear=&pid='),link))\n",
        "            target_link_list = target_link_list + link                                                                                                      #將生成的網址存入target_link_list\n",
        "            time.sleep(delay)                                                                                                                               #隨機休眠1~5秒\n",
        "        return target_link_list                                                                                                                             #返回目標基金總覽網址清單\n",
        "\n",
        "    def retry_page(self, url):                                                                                                                              #宣告 retry_page()，此function需要輸入一個參數，為網址\n",
        "        times = 0                     \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t                                                            #宣告times變數為0\n",
        "        while times<3:                                                                                                                                      #透過while迴圈，當取得網頁內容出錯時會重新嘗試取得內容，最多嘗試三次                                                     \n",
        "            try:\n",
        "                res=requests.get(url,headers = header)\n",
        "                soup = BeautifulSoup(res.text, 'html.parser')\n",
        "                return soup\n",
        "            except:\n",
        "                times+=1\n",
        "                print(f\"try {times} time, {url}\")\n",
        "                pass\n",
        "\n",
        "\n",
        "    #這三個def架構為 meta_link_generator() >> changing_page() >> ID_crawler()\n",
        "    def ID_crawler(self, url, ID_link):\t\t\t\t\t\t            \t\t\t\t                                                                    #宣告 ID_crawler()，此function需要輸入兩個參數，分別為網址與目標機構代碼列表\n",
        "        main = 'main1.jsp?fundId='\t\t\t\t\t\t\t\t\t\t\t\n",
        "        soup = self.retry_page(url)\t\t\t\t\t\t\t\t\t\t\t                                                                                #嘗試取得網頁內容\n",
        "        fundID = soup.select('tr.row1 td[style=\"cursor: hand;\"]')+ soup.select('tr.row2 td[style=\"cursor: hand;\"]')                                         #取得包含各檔基金代碼的元素\n",
        "        regex4 = re.compile(\"A01_02.jsp\\?fundId=.+',\")\t\t\t\t\t\t\t\t\t                                                                    #定義正規表達式A01_02.jsp\\?fundId=.+',並存入regex4變數，目的為生成基金基本資訊網址\n",
        "        link2 = [regex4.search(str(s)).group(0) for s in fundID if regex4.search(str(s))]\t\t\t\t                                                    #篩選出符合regex4的元素，並將結果存入link2\n",
        "        _ID_link = list(map(lambda x : link_header+main+x.replace(\"',\",\"\").split('=')[1],link2))\t\t\t                                                #_ID_link為當前頁面各檔基金的基本資訊頁面連結\n",
        "        ID_link = ID_link+_ID_link\t\t\t\t\t\t\t\t\t\t\t\n",
        "        TdRight = soup.select('td[align=\"right\"]')\n",
        "        regex3 = re.compile(\"(.*)\")    \n",
        "        summary = regex3.search(str(TdRight)).group(0)\n",
        "        try:\n",
        "            full_page = int(re.search('\\d{1,2}',summary.split(',')[0]).group(0))\t\t\t\t\t                                                        #full_page為此機構基金列表的總頁數，無其他分頁則讓#full_page等於1\n",
        "        except:\n",
        "            full_page = 1\n",
        "        time.sleep(delay)\t\t\t\t\t\t\t\t\t\t\t\t                                                                                    #隨機休眠1~5秒\n",
        "        return (url, full_page, ID_link)\t\t\t\t\t\t\t\t\t\t                                                                            #返回當前頁面網址、總頁數、基金基本資訊網址清單\n",
        "\n",
        "    def changing_page(self, firsturl, ID_link):\t\t\t\t\t\t\t\t    #宣告 changing_page()，此function需要輸入兩個參數，分別為機構基金列表的第一頁網址與基金基本資訊網址清單\n",
        "        i=1\t\t\t\t\t\t\t\t\t\t\t\t\t\t                #定義變數i為1\n",
        "        url = firsturl+str(i)\t\t\t\t\t\t\t\t\t\t\t\t    #爬蟲網址為機構基金列表的第一頁網址加上i\n",
        "        print(url)\t\t\t\t\t\t\t\t\t\t\t\t\t            #列印當前網址\n",
        "        temp = self.ID_crawler(url,ID_link)\t\t\t\t\t\t\t\t\t\t#將 ID_crawler() 的結果存入變數temp\n",
        "        ID_link = temp[2]\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "        end_page = temp[1]\n",
        "        url = temp[0]\n",
        "        print(f'page1/{end_page}')\n",
        "\n",
        "        for j in range(2,int(end_page)+1):\t\t\t\t\t\t\t\t\t\t#透過 for 迴圈實現爬蟲可自動換頁爬取直到最後一頁\n",
        "            url = url.split('pid=',1)[0]+'pid='+str(j)\n",
        "            print(url)\n",
        "\n",
        "            sub_temp = self.ID_crawler(url,ID_link)\n",
        "            ID_link=sub_temp[2]\n",
        "            print(f'page{j}/{end_page}')\t\t\t\t\t\t\t\t\t\t#列印出目前爬蟲進度\n",
        "            time.sleep(delay)\t\t\t\t\t\t\t\t\t\t\t\t    #隨機休眠1~5秒\n",
        "        return ID_link\t\t\t\t\t\t\t\t\t\t\t\t\t        #返回基金基本資訊網址清單\n",
        "\n",
        "    def meta_link_generator(self, target_link_list):\t\t\t\t\t\t\t#宣告 meta_link_generator()，此function需要輸入一個變數，為目標基金總覽網址清單\n",
        "        a = []\n",
        "        ID_link = []\t\t\t\t\t\t\t\t\t\t\t\t\t        #定義變數ID_link為空list\n",
        "        for x,y in enumerate(target_link_list):\t\t\t\t\t\t\t\t\t#透過 for 迴圈達成針對目標機構爬取基金資訊\n",
        "            a = a + self.changing_page(y, ID_link)\n",
        "            print(f'{x+1}/{len(target_link_list)}')\t\t\t\t\t\t\t\t#列印出目前爬蟲頁數\n",
        "        return a\t\t\t\t\t\t\t\t\t\t\t\t\t            #返回全部基金基本資訊網址\n",
        "\n",
        "\n",
        "    def Replace(self, x):\t\t\t\t\t\t\t\t\t\t\t\t                                        #宣告 Replace()，此function需要輸入一個變數，為經過BeautifulSoup解析過的網頁元素\n",
        "        x = x.text\t\t\t\t\t\t\t\t\t\t\t\t\t                                            #篩選元素中的純文字訊息\n",
        "        x = x.replace('\\t','').replace('\\n','').replace('\\xa0','').replace('\\r','').replace('\\\"','')\t\t\t#將空白字元移除\n",
        "        return x\n",
        "\n",
        "    def ToJSON(self, keys, values):\t\t\t\t\t\t\t\t\t\t\t\t#宣告 ToJSON()，此function需要輸入兩個參數，分別為key值的list與value值的list\n",
        "        a = list(map(lambda x : self.Replace(x), keys))                         \n",
        "        b = list(map(lambda x : self.Replace(x), values))\n",
        "        res = {a[i]: b[i] for i in range(len(keys))}\n",
        "        json_object = json.loads(json.dumps(res, ensure_ascii=False))           #json.dumps回傳type為str，要用;json.loads()轉為dict\n",
        "        return json_object\n",
        "\n",
        "    def Convert(self, fund_name):                                               #宣告 Convert()，此function需要輸入一個變數，為基金名稱\n",
        "        name = '基金名稱'\n",
        "        res_dct = {name : fund_name}\n",
        "        res_dct = json.loads(json.dumps(res_dct, ensure_ascii=False))           #json.dumps回傳type為str，要用;json.loads()轉為dict\n",
        "        return res_dct\n",
        "\n",
        "    def get_source(self, url):                                                  #宣告 get_source()，此function需要輸入一個變數，為網址\n",
        "        times = 0\n",
        "        while times<3 :                                                         #透過while迴圈，當取得網頁內容出錯時會重新嘗試取得內容，最多嘗試三次 \n",
        "            try:\n",
        "                res = requests.get(url, headers=header)\n",
        "                soup = BeautifulSoup(res.text, 'html.parser')\n",
        "                c_name = soup.select('font[color=\"#003399\"]')[0]\n",
        "                return (soup, c_name)\n",
        "            except:\n",
        "                times+=1\n",
        "                print(f\"try {times} time, {url}\")\n",
        "                pass\n",
        "\n",
        "\n",
        "    def meta_crawler(self, link_list):                                          #宣告 meta_crawler()，此function需要輸入一個變數，為基金基本資訊網址的list\n",
        "        with open(f'{err_path}{bug_url}.txt', mode='w') as D:                   #在err_path的資料夾下新增一個錯誤連結記錄檔，模式為寫入\n",
        "            D.write(\"Error url:\\n\")\n",
        "            D.close()\n",
        "        json_list=[]\n",
        "        for num, i in enumerate(link_list):                                     #透過 for 迴圈針對所有基金基本資訊連結進行資料爬取\n",
        "            try:\n",
        "                soup, c_name = self.get_source(i)\n",
        "                c = self.Convert(self.Replace(c_name))\n",
        "                title = soup.select('.FieldTitle')                              #篩選出細項標題\n",
        "                content = soup.select('.FieldContent')                          #篩選出細項內容\n",
        "                j = self.ToJSON(title, content)                                 \n",
        "                c.update(j)                                                     \n",
        "                with open(f'{txt_path}{filename}{month}.txt', mode='a') as T:   #將爬取資料寫入txt檔中\n",
        "                    T.write(str(c))\n",
        "                    if num+1 == len(link_list):\n",
        "                      pass\n",
        "                    else:\n",
        "                      T.write(',')\n",
        "                      \n",
        "                json_list.append(c)\n",
        "                time.sleep(delay)\n",
        "            except:\n",
        "                with open(f'{err_path}{bug_url}.txt', mode='a') as D:           #如果爬取失敗將失敗連結寫入錯誤連結記錄檔\n",
        "                            D.write(f'{i},')\n",
        "                            D.write(\"\\n\")\n",
        "                            D.close() \n",
        "                print(f\"{i} got problem\")\n",
        "                pass      \n",
        "            #測試\n",
        "            if num ==10:\n",
        "              break\n",
        "\n",
        "            print(f'{self.Replace(c_name)}, {num+1}/{len(link_list)}')\n",
        "        with open(f'{txt_path}{filename}{month}.txt', mode='a') as T:\n",
        "            T.write(']}')\n",
        "            T.close()       \n",
        "        return json_list\n",
        "\n",
        "    def toDataframe(self, json_list):                                           #宣告 toDataframe()，此function需要輸入一個變數，為各檔基金資料匯集成的list\n",
        "        df = pd.DataFrame(json_list,columns=list(json_list[0].keys()))          #將list of json 的資料轉換為pandas的dataframe\n",
        "        return df\n",
        "\n",
        "    def store(self, df):                                                         #宣告 store()，此function需要輸入一個變數，為dataframe\n",
        "        df.to_csv(f'{csv_path}{filename}{month}.csv', encoding='utf-8')             #匯出csv\n",
        "        df.to_csv(f'{newest_path}{filename}.csv', encoding='utf-8')                 #匯出csv\n",
        "\n",
        "    # target_list = ['A0038','A0031','A0018','B0029','A0036','A0011','A0032']\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    c = newForeignFund()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HCB323SYO55T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}